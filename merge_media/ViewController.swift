
//
//  ViewController.swift
//  merge_media
//
//  Created by dengjiangzhou on 2018/8/20.
//  Copyright Â© 2018å¹´ dengjiangzhou. All rights reserved.
//

import UIKit
import AVFoundation
import AVKit

import MobileCoreServices
import Photos
import MediaPlayer

class ViewController: UIViewController {

    
    @IBOutlet weak var videoTable: UITableView!
    @IBOutlet weak var activityIndicator: UIActivityIndicatorView!
    
    
    var imagePicker: UIImagePickerController!
    var videoURLs = [URL]()
//
    var currentTableIndex = 0
    var HDVideoSize: CGSize{
        return self.view.bounds.size
    }
    
    var musicAsset: AVAsset?
    var previewURL: URL?
    
    override func viewDidLoad() {
        super.viewDidLoad()
        // Do any additional setup after loading the view, typically from a nib.
    }

    // MARK:- ç¬¬ä¸€ä¸ªæŒ‰é’®
    @IBAction func addVideoClip(_ sender: UIButton) {
        guard activityIndicator.isAnimating == false else{
            return
        }
        
        refreshURL()
        
        imagePicker = UIImagePickerController()
        imagePicker.delegate = self
        imagePicker.sourceType = .photoLibrary
        imagePicker.allowsEditing = false
        imagePicker.mediaTypes = [kUTTypeMovie as String]
        imagePicker.videoQuality = .typeHigh
        present(imagePicker, animated: true)
    }
    
    
    
    // MARK:- ç¬¬äºŒä¸ªæŒ‰é’®
    @IBAction func deleteVideoClip(_ sender: UIButton) {
        guard currentTableIndex != -1 , activityIndicator.isAnimating == false else {
            return
        }
        
        refreshURL()
        
        let theAlert = UIAlertController(title: "Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð²Ð¸Ð´ÐµÐ¾", message: "Ð’Ñ‹ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ð¾Ðµ Ð²Ð¸Ð´ÐµÐ¾", preferredStyle: .alert)
        theAlert.addAction(UIAlertAction(title: "ÐÐµÑ‚", style: .cancel))
        theAlert.addAction(UIAlertAction(title: "Ð”Ð°", style: .destructive, handler: { (action: UIAlertAction) in
            self.videoURLs.remove(at: self.currentTableIndex)
            self.videoTable.reloadData()
            self.currentTableIndex = 0
            self.previewURL = nil
        }))
        present(theAlert, animated: true)
        
    }
    
    @IBAction func previewComposition(_ sender: UIButton) {
        
        
        
        guard videoURLs.count > 0 , activityIndicator.isAnimating == false else{
            return
        }
        var player: AVPlayer!
        defer {

            
            player = AVPlayer(url: videoURLs[currentTableIndex])
            
            let playerViewController = AVPlayerViewController()
            playerViewController.allowsPictureInPicturePlayback = true
            playerViewController.player = player
            
            present(playerViewController, animated: true) {
                playerViewController.player!.play()
            }
        }
        
        guard previewURL == nil else {
            
            //  if previewURL == nil,
            //  you will be creating the AVPlayer item from the composition.
            player = AVPlayer(url: previewURL!)
            return
        }
        
        //  get assets for video clips
        
        var videoAssets = [AVAsset]()
        for urlOne in videoURLs{
            let av_asset = AVAsset(url: urlOne)
            videoAssets.append(av_asset)
        }// Get assets for video clips
        
        // Create AVMutableComposition to hold AVMutableCompositionTrack instances.
        let composition = AVMutableComposition()
        
        // Create tracks from assets.
        let videoTrack = composition.addMutableTrack(withMediaType: .video, preferredTrackID: kCMPersistentTrackID_Invalid)
        let audioTrack = composition.addMutableTrack(withMediaType: .audio, preferredTrackID: kCMPersistentTrackID_Invalid)
        var startTime = kCMTimeZero
        
        //  iterates through the video assets array and
        //  inserted time range of media from each asset into both video and audio tracks.
        
        //
        let mainInstruction = AVMutableVideoCompositionInstruction()
//        for asset in videoAssets{
        
            do{
                // Insert Video
                try videoTrack?.insertTimeRange(CMTimeRangeMake(kCMTimeZero, videoAssets[currentTableIndex].duration), of: videoAssets[currentTableIndex].tracks(withMediaType: .video)[0], at: startTime)
            }catch{
                return
            }
            
            // Insert Audio
            if musicAsset == nil {
                do{
                    // Insert Audio
                    try audioTrack?.insertTimeRange(CMTimeRangeMake(kCMTimeZero, videoAssets[currentTableIndex].duration), of: videoAssets[currentTableIndex].tracks(withMediaType: .audio)[0], at: startTime)
                }catch{
                    return
                }
//                //  The start time is updated,
//                //  so that the next asset begins after the previous.
            }
            let instruction = videoCompositionInstructionForTrack(track: videoTrack!, asset: videoAssets[currentTableIndex])
            instruction.setOpacity(1.0, at: startTime)
            mainInstruction.layerInstructions.append(instruction)
            startTime = CMTimeAdd(startTime, videoAssets[currentTableIndex].duration)
            // The start time is updated ,
            //  so that the next asset begins after the previous
//        }
        let totalDuration = startTime
        if musicAsset != nil {
            do{
                try audioTrack?.insertTimeRange(CMTimeRangeMake(kCMTimeZero, totalDuration), of: musicAsset!.tracks(withMediaType: .audio)[0], at: kCMTimeZero)
            }
            catch{
                print("Error creating soundtrack total.")
            }
            //  This adds the audio from the musicAsset to the audio track for the entire duration of the composition.
        }
        
        mainInstruction.timeRange = CMTimeRangeMake(kCMTimeZero, totalDuration)
        let videoComposition = AVMutableVideoComposition()
        videoComposition.instructions = [mainInstruction]
        videoComposition.frameDuration = CMTimeMake(1, 30)
        videoComposition.renderSize = HDVideoSize
        videoComposition.renderScale = 1.0
        
        //  https://stackoverflow.com/questions/32081057/how-to-convert-an-avmutablevideocomposition-to-an-avasset
        let playItem = AVPlayerItem(asset: composition)
        playItem.videoComposition = videoComposition
        player = AVPlayer(playerItem: playItem)
    }
    
    
    func refreshURL() {
        
        guard videoURLs.count > 0  else{
            return
        }
        
        self.previewURL = nil
        
        
    }
    
    // MARK:- åˆæˆè§†é¢‘  ï¼ˆ éŸ³è§†é¢‘ ï¼‰
    // the merge and export process can take some time,
    private func have_a_spin_mergeAndExportVideo(){

        activityIndicator.isHidden = false
        
        //  start animating the activity indicator,
        //  to provide visual feedback that something ia happening.
        //  ç»™ç”¨æˆ·åé¦ˆ
        activityIndicator.startAnimating()
        
        //  æŠŠè®°å½•çš„ previewURL ç½®ä¸º nil
        //  è§†é¢‘åˆæˆï¼Œ å¯¼å‡ºæˆåŠŸï¼Œ å°±èµ‹æ–°å€¼
        previewURL = nil
        
        // å…ˆåˆ›å»ºèµ„æº AVAsset
        var videoAssets = [AVAsset]()
        for url_piece in videoURLs{
            let av_asset = AVAsset(url: url_piece)
            videoAssets.append(av_asset)
        }
        // åˆ›å»ºåˆæˆçš„ AVMutableComposition å¯¹è±¡
        let composition = AVMutableComposition()
        //  åˆ›å»º AVMutableComposition å¯¹è±¡çš„éŸ³è½¨
        let audioTrack = composition.addMutableTrack(withMediaType: .audio, preferredTrackID: Int32(kCMPersistentTrackID_Invalid))
        
        // é€šè¿‡ AVMutableVideoCompositionInstruction ï¼Œè°ƒæ•´åˆæˆè½¨è¿¹çš„æ¯”ä¾‹ã€ä½ç½®ã€è£å‰ªå’Œé€æ˜Žåº¦ç­‰å±žæ€§ã€‚
        // AVMutableVideoCompositionInstruction å¯¹è±¡ï¼Œ æŽ§åˆ¶ä¸€ç»„ layer å¯¹è±¡ AVMutableVideoCompositionLayerInstruction
        let mainInstruction = AVMutableVideoCompositionInstruction()
        var startTime = kCMTimeZero
        // iterating through the video assets to add content to the AVMutableComposition
        for asset in videoAssets{
            //  Insert Video

            //  instead of inserting segments of video . into a single video track
            //  this time , you are creaing video track for every video asset.
            
            // this is because the layer instructions are applied to an entire track.
            let videoTrack = composition.addMutableTrack(withMediaType: .video, preferredTrackID: Int32(kCMPersistentTrackID_Invalid))
            do{
                try videoTrack?.insertTimeRange(CMTimeRangeMake(kCMTimeZero, asset.duration), of: asset.tracks(withMediaType: .video)[0], at: startTime)
            }catch{
                print("Error creating Video track.")
            }
            
            // Insert Audio
            if musicAsset == nil {
                //  This code prevents audio from the video assets from being added to the audio track, effectively muting the sound from the video clips.

                
                do{
                    try audioTrack?.insertTimeRange(CMTimeRangeMake(kCMTimeZero, asset.duration), of: asset.tracks(withMediaType: .audio)[0], at: startTime)
                }
                catch{
                    print("Error creating Audio track.")
                }
                
            }   //  if musicAsset == nil for asset in videoAssets{
            
            

            let instruction = videoCompositionInstructionForTrack(track: videoTrack!, asset: asset)
            instruction.setOpacity(1.0, at: startTime)
            if asset != videoAssets.last{
                instruction.setOpacity(0.0, at: CMTimeAdd(startTime, asset.duration))
                //  at the end of each video assets duration,
                //  the opacity is set to 0
                //  to prevent the track from displaying past in time.
                
                //  æ¯ä¸€ä¸ªè§†é¢‘ç‰‡æ®µç»“æŸï¼Œ éƒ½æ·»åŠ äº†è¿‡æ¸¡, é¿å…ç‰‡æ®µä¹‹é—´çš„å¹²æ¶‰
                // other wise, the tracks would obscure each other.
                
            }
            mainInstruction.layerInstructions.append(instruction)
            // after adding our assets , one layerInstruction is created for
            // every video track to correct its orientation ,
            // using helper method videoCompositionInstructionForTrack
            
            
            // è¿™æ ·ï¼Œ mainInstruction å°±æ·»åŠ å¥½äº†
            startTime = CMTimeAdd(startTime, asset.duration)
        }// for asset in videoAssets{
        
        let totalDuration = startTime
        if musicAsset != nil {
            do{
                try audioTrack?.insertTimeRange(CMTimeRangeMake(kCMTimeZero, totalDuration), of: musicAsset!.tracks(withMediaType: .audio)[0], at: kCMTimeZero)
            }
            catch{
                print("Error creating soundtrack total.")
            }
            //  This adds the audio from the musicAsset to the audio track for the entire duration of the composition.
        }
        // sets the mainInstruction timeRange,
        //  to the total duration of all our video assets.
        
        mainInstruction.timeRange = CMTimeRangeMake(kCMTimeZero, totalDuration)
        
        
        //  an AVMutableVideoComposition , provides a description of
        //  how multiple video tracks
        //  are composited together at any point along the composition's timeline
        
        //  è§†é¢‘ä»¬å¦‚ä½•åˆæˆ
        
        // it also provides a way to configure
        //  its frame , duration, render size ,and scale
        
        let videoComposition = AVMutableVideoComposition()
        videoComposition.instructions = [mainInstruction]
        videoComposition.frameDuration = CMTimeMake(1, 30)
        videoComposition.renderSize = HDVideoSize
        videoComposition.renderScale = 1.0
        
        
        // Export
        //  æ‹¿ composition ï¼Œåˆ›å»º AVAssetExportSession
        let exporter: AVAssetExportSession = AVAssetExportSession(asset: composition, presetName: AVAssetExportPresetHighestQuality)!
        // é…ç½®è¾“å‡ºçš„ url
        exporter.outputURL = theURL
        // è®¾å®šè¾“å‡ºæ ¼å¼ï¼Œ quick time movie file
        exporter.outputFileType =  AVFileType.mp4
        //  optimizing it to play on the internet
        exporter.shouldOptimizeForNetworkUse = true
        exporter.videoComposition = videoComposition
        // start the export session
        //  å¼€å¯ä¼šè¯ï¼Œ å»ºç«‹è¿žæŽ¥
        exporter.exportAsynchronously {
            DispatchQueue.main.async {
                self.exportDidFinish_deng(session: exporter)
            }
        }
    }
    
    
    
    
    override func didReceiveMemoryWarning() {
        super.didReceiveMemoryWarning()
        // Dispose of any resources that can be recreated.
    }


}



// MARK:- Helper
extension ViewController{
    func addVideoURL(url: URL){
        videoURLs.append(url)
        videoTable.reloadData()
        previewURL = nil
    }
    
    //  which saves the video to the shared photo library
    //
    func exportDidFinish_deng(session: AVAssetExportSession){
        defer {
            activityIndicator.stopAnimating()
            activityIndicator.isHidden = true
        }

        
        guard session.status == .completed else{
            return
        }
        
        let outputURL = session.outputURL
        let photoLibrary = PHPhotoLibrary.shared()
        photoLibrary.performChanges({
             PHAssetChangeRequest.creationRequestForAssetFromVideo(atFileURL: outputURL!)
        }, completionHandler: {
            (success: Bool , error: Error?)
            in
            var alertTitle: String!
            var alertMessage: String!
            if success{
                alertTitle = "Ð£ÑÐ¿ÐµÑ…!"
                alertMessage = "Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð²Ð¸Ð´ÐµÐ¾ Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾"
                self.previewURL = session.outputURL
            }
            else{
                alertTitle = "Ð£Ð¿ÑðŸ¤ª"
                alertMessage = "Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð²Ð¸Ð´ÐµÐ¾ Ð½Ðµ Ð²Ñ‹ÑˆÐ»Ð¾"
                self.previewURL = nil
            }
            let alert = UIAlertController(title: alertTitle, message: alertMessage, preferredStyle: .alert)
            alert.addAction(UIAlertAction(title: "ÐžÐº", style: .cancel))
            DispatchQueue.main.async {
                self.present(alert, animated: true)
            }
        })
    }
    
    
    // Deng : Finish add
    // This creates a layer instruction from the passed  in track
    //  and grabs the video from the AVAsset
    
    func videoCompositionInstructionForTrack(track: AVCompositionTrack, asset: AVAsset) -> AVMutableVideoCompositionLayerInstruction{
        let instruction = AVMutableVideoCompositionLayerInstruction(assetTrack: track)
        let assetTrack = asset.tracks(withMediaType: .video)[0]
        // here , you get the asset's preferredTransform
        //  and use it to determine if the video is portrait or landscape , using the helper method orientationFromTransform()
        let transfrom = assetTrack.preferredTransform
        let assetInfo = transfrom.orientationFromTransform()
        
        //  the scaleToFitRatio , is set to match a high definition video landscape orientation
        var scaleToFitRatio = HDVideoSize.width / assetTrack.naturalSize.width
        
        if assetInfo.isPortrait  {
            // Portrait
            
            //  change the scale to fit ratio to accommodate a high definition video portrait orientation
            
            scaleToFitRatio = HDVideoSize.height / assetTrack.naturalSize.width
            
            // then create a scale transform matrix
            let scaleFactor = CGAffineTransform(scaleX: scaleToFitRatio, y: scaleToFitRatio)

            let concatTranform = assetTrack.preferredTransform.concatenating(scaleFactor)
            instruction.setTransform(concatTranform, at: kCMTimeZero)
            
            
        }
        else{
            // Landscape
            
            // rotate and scale if neccessary
            let scale_factor = CGAffineTransform(scaleX: scaleToFitRatio, y: scaleToFitRatio)
            let scale_factor_two = CGAffineTransform(rotationAngle: .pi/2.0)
            let concat_transform = assetTrack.preferredTransform.concatenating(scale_factor).concatenating(scale_factor_two)
            instruction.setTransform(concat_transform, at: kCMTimeZero)

        }
        // å°†å¤„ç†å¥½çš„ AVMutableVideoCompositionLayerInstruction è¿”å›ž
        return instruction
    }
}



extension ViewController: UIImagePickerControllerDelegate, UINavigationControllerDelegate{
    func imagePickerController(_ picker: UIImagePickerController, didFinishPickingMediaWithInfo info: [String : Any]) {
        let theImagePath: URL = info["UIImagePickerControllerMediaURL"] as! URL
        print(info["UIImagePickerControllerPHAsset"] ?? "")
        addVideoURL(url: theImagePath)
        imagePicker.dismiss(animated: true)
        imagePicker = nil
    }
    
    
    func imagePickerControllerDidCancel(_ picker: UIImagePickerController) {
        imagePicker.dismiss(animated: true)
        imagePicker = nil
    }
    
}



extension ViewController: UITableViewDelegate{
    func tableView(_ tableView: UITableView, didSelectRowAt indexPath: IndexPath) {
        currentTableIndex = indexPath.row
    }
    
    
}



extension ViewController: UITableViewDataSource{
    
    func tableView(_ tableView: UITableView, numberOfRowsInSection section: Int) -> Int {
        return videoURLs.count
    }
    
    
    func tableView(_ tableView: UITableView, cellForRowAt indexPath: IndexPath) -> UITableViewCell {
        let cell = tableView.dequeueReusableCell(withIdentifier: "VideoClipCell") as! VideoTableViewCell
        cell.clipName.text = "Ð’Ð¸Ð´ÐµÐ¾ï¼Œ \(indexPath.row + 1)"
        cell.clipThumbnail.image = videoURLs[indexPath.row].previewImageFromVideo()
        return cell
    }
    
    
}
